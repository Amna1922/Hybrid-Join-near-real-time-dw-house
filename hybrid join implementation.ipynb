{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d3989eb",
   "metadata": {},
   "source": [
    "pt2.of hybrid join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ebdfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "import threading\n",
    "import time\n",
    "from collections import deque, defaultdict\n",
    "import sys\n",
    "\n",
    "class HYBRIDJOIN:\n",
    "    def __init__(self, db_credentials, hS=10000, vP=500):\n",
    "        \"\"\"\n",
    "        HYBRIDJOIN Algorithm with exact parameters from project specs\n",
    "        hS = 10,000 slots (hash table compartments)\n",
    "        vP = 500 tuples (disk partition size)\n",
    "        \"\"\"\n",
    "        # STEP 1: SETUP TOOLS (Exact parameters from specs)\n",
    "        \n",
    "        self.hS = hS  # 10,000 slots as specified\n",
    "        self.vP = vP  # 500 tuples per disk partition as specified\n",
    "        \n",
    "        # Hash Table Multimap with exactly hS compartments\n",
    "        self.hash_table = defaultdict(list)  # Allows multiple entries per key\n",
    "        \n",
    "        # Queue: FIFO order for fairness in processing\n",
    "        self.queue = deque()  # Stores join attribute values (keys)\n",
    "        \n",
    "        # Disk Buffer: Memory buffer for disk partitions of size vP\n",
    "        self.disk_buffer = []  # Join window - holds vP tuples from R\n",
    "        \n",
    "        # Stream Buffer: Temporary hold for bursty stream data\n",
    "        self.stream_buffer = []  # Prevents loss of data in bursty scenarios\n",
    "        \n",
    "        # Stream Input (w): Available slots in hash table\n",
    "        self.w = hS  # Starts with all hS compartments free\n",
    "        \n",
    "        # Algorithm control\n",
    "        self.running = True\n",
    "        self.processed_count = 0\n",
    "        self.stream_finished = False\n",
    "        \n",
    "        # Database connection for disk-based relation R\n",
    "        self.conn = pymysql.connect(**db_credentials)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.cursor.execute(\"USE walmart\")\n",
    "        \n",
    "        # Pre-load disk-based relation R (Master Data)\n",
    "        self.load_disk_relation()\n",
    "        \n",
    "        print(f\"STEP 1 COMPLETED: Tools initialized with hS={hS}, vP={vP}, w={self.w}\")\n",
    "\n",
    "    def load_disk_relation(self):\n",
    "        \"\"\"Load disk-based relation R (Master Data) into memory\"\"\"\n",
    "        print(\"Loading disk-based relation R (Master Data)...\")\n",
    "        \n",
    "        # Customer master data - part of disk relation R\n",
    "        self.customer_master = {}\n",
    "        self.cursor.execute(\"SELECT Customer_ID, Gender, Age, Occupation, City_Category, Stay_In_Current_City_Years, Marital_Status FROM DimCustomer\")\n",
    "        for row in self.cursor.fetchall():\n",
    "            self.customer_master[row[0]] = {\n",
    "                'Gender': row[1], 'Age': row[2], 'Occupation': row[3],\n",
    "                'City_Category': row[4], 'Stay_In_Current_City_Years': row[5],\n",
    "                'Marital_Status': row[6]\n",
    "            }\n",
    "        \n",
    "        # Product master data - part of disk relation R\n",
    "        self.product_master = {}\n",
    "        self.cursor.execute(\"SELECT Product_ID, Product_Category, Price, StoreID, SupplierID, StoreName, SupplierName FROM DimProduct\")\n",
    "        for row in self.cursor.fetchall():\n",
    "            self.product_master[row[0]] = {\n",
    "                'Product_Category': row[1], 'Price': row[2], \n",
    "                'StoreID': row[3], 'SupplierID': row[4],\n",
    "                'StoreName': row[5], 'SupplierName': row[6]\n",
    "            }\n",
    "        \n",
    "        print(f\"Disk relation R loaded: {len(self.customer_master)} customers, {len(self.product_master)} products\")\n",
    "\n",
    "    def stream_reader_thread(self):\n",
    "        \"\"\"\n",
    "        Independent thread: Continuously gets data from transactional_data.csv\n",
    "        into stream buffer as specified in project notes\n",
    "        \"\"\"\n",
    "        print(\"Stream reader thread started - continuously feeding stream S\")\n",
    "        \n",
    "        try:\n",
    "            # Read transactional data in appropriate chunks for near-real-time simulation\n",
    "            chunk_size = 200  # Balanced chunk size for performance\n",
    "            stream_chunks = 0\n",
    "            \n",
    "            for chunk in pd.read_csv('transactional_data.csv', chunksize=chunk_size):\n",
    "                if not self.running:\n",
    "                    break\n",
    "                \n",
    "                transactions = chunk.to_dict('records')\n",
    "                self.stream_buffer.extend(transactions)\n",
    "                stream_chunks += 1\n",
    "                \n",
    "                print(f\"Stream S → Stream Buffer: +{len(transactions)} tuples (Total: {len(self.stream_buffer)})\")\n",
    "                time.sleep(0.02)  # Near-real-time simulation delay\n",
    "            \n",
    "            self.stream_finished = True\n",
    "            print(f\"STREAM READING COMPLETED: Processed {stream_chunks} chunks\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Stream reader error: {e}\")\n",
    "            self.stream_finished = True\n",
    "\n",
    "    def hash_function(self, key):\n",
    "        \"\"\"Hash function mapping join keys to hS compartments\"\"\"\n",
    "        return hash(str(key)) % self.hS\n",
    "\n",
    "    def hybrid_join_algorithm(self):\n",
    "        \"\"\"\n",
    "        Main HYBRIDJOIN algorithm thread\n",
    "        Implements the exact 5-step process from project specifications\n",
    "        \"\"\"\n",
    "        print(\"HYBRIDJOIN algorithm thread started\")\n",
    "        \n",
    "        iteration = 0\n",
    "        max_empty_iterations = 100  # Stop after 100 empty iterations\n",
    "        \n",
    "        while self.running and iteration < 100000:  # Safety limit\n",
    "            iteration += 1\n",
    "            \n",
    "            # STEP 2: STREAM BUFFER → HASH TABLE + QUEUE\n",
    "            \n",
    "            stream_processed = 0\n",
    "            if self.stream_buffer and self.w > 0:\n",
    "                # Take up to w pieces of data as specified\n",
    "                tuples_to_process = min(self.w, len(self.stream_buffer))\n",
    "                stream_processed = 0\n",
    "                \n",
    "                for i in range(tuples_to_process):\n",
    "                    if not self.stream_buffer:\n",
    "                        break\n",
    "                    \n",
    "                    stream_tuple = self.stream_buffer.pop(0)\n",
    "                    join_key = stream_tuple['Customer_ID']  # Join attribute\n",
    "                    \n",
    "                    # Hash to pick compartment\n",
    "                    slot = self.hash_function(join_key)\n",
    "                    \n",
    "                    # Store in hash table (multi-map structure)\n",
    "                    hash_entry = {\n",
    "                        'tuple': stream_tuple,\n",
    "                        'key': join_key,\n",
    "                        'queue_reference': len(self.queue)  # Pointer to queue node\n",
    "                    }\n",
    "                    self.hash_table[slot].append(hash_entry)\n",
    "                    \n",
    "                    # Add key to queue (FIFO order)\n",
    "                    self.queue.append(join_key)\n",
    "                    stream_processed += 1\n",
    "                \n",
    "                # w becomes smaller as specified\n",
    "                self.w -= stream_processed\n",
    "                \n",
    "                if stream_processed > 0:\n",
    "                    print(f\"STEP 2: Loaded {stream_processed} tuples to hash table. w={self.w}, Queue size: {len(self.queue)}\")\n",
    "\n",
    "            # STEP 3: QUEUE → DISK BUFFER (Join Window)\n",
    "            \n",
    "            disk_loaded = 0\n",
    "            if self.queue:\n",
    "                # Use oldest key from queue (FIFO order)\n",
    "                oldest_key = self.queue[0]\n",
    "                \n",
    "                # Load disk partition of size vP for this key\n",
    "                self.disk_buffer = self.load_disk_partition(oldest_key)\n",
    "                disk_loaded = len(self.disk_buffer)\n",
    "                \n",
    "                if disk_loaded > 0:\n",
    "                    print(f\"STEP 3: Loaded disk partition for key {oldest_key} (vP={disk_loaded})\")\n",
    "\n",
    "            # STEP 4: JOIN PROCESSING → DATA WAREHOUSE\n",
    "            \n",
    "            join_output_count = 0\n",
    "            freed_slots = 0\n",
    "            \n",
    "            if self.disk_buffer and self.queue:\n",
    "                oldest_key = self.queue[0]\n",
    "                \n",
    "                # Probe hash table with disk buffer tuples\n",
    "                for disk_tuple in self.disk_buffer:\n",
    "                    slot = self.hash_function(oldest_key)\n",
    "                    matching_entries = [e for e in self.hash_table[slot] if e['key'] == oldest_key]\n",
    "                    \n",
    "                    # Process up to vP tuples as specified\n",
    "                    for hash_entry in matching_entries[:self.vP]:\n",
    "                        stream_tuple = hash_entry['tuple']\n",
    "                        \n",
    "                        # Generate join output (combined result)\n",
    "                        enriched_data = self.generate_join_output(stream_tuple, disk_tuple)\n",
    "                        if enriched_data:\n",
    "                            # Load to Data Warehouse\n",
    "                            if self.load_to_dw(enriched_data):\n",
    "                                # Remove matched data from hash table\n",
    "                                self.hash_table[slot].remove(hash_entry)\n",
    "                                freed_slots += 1\n",
    "                                join_output_count += 1\n",
    "                                self.processed_count += 1\n",
    "                \n",
    "                # Update w with freed spaces\n",
    "                self.w += freed_slots\n",
    "                \n",
    "                # Remove processed keys from queue\n",
    "                self.cleanup_queue()\n",
    "                \n",
    "                if join_output_count > 0:\n",
    "                    print(f\"STEP 4: Joined {join_output_count} records. Freed {freed_slots} slots. Total processed: {self.processed_count}\")\n",
    "\n",
    "            # STEP 5: LOOP BACK TO STEP 2\n",
    "            \n",
    "            # Check completion conditions\n",
    "            if self.stream_finished and not self.stream_buffer and not self.queue:\n",
    "                print(\"ALL DATA PROCESSED - HYBRIDJOIN COMPLETED!\")\n",
    "                break\n",
    "                \n",
    "            # If no progress in this iteration, small delay\n",
    "            if stream_processed == 0 and disk_loaded == 0 and join_output_count == 0:\n",
    "                time.sleep(0.01)\n",
    "                if iteration % 1000 == 0:\n",
    "                    print(f\"Progress: {self.processed_count} records, Buffer: {len(self.stream_buffer)}, Queue: {len(self.queue)}\")\n",
    "            else:\n",
    "                time.sleep(0.001)  # Minimal delay when processing\n",
    "        \n",
    "        print(f\"HYBRIDJOIN FINAL: {self.processed_count} records loaded to Data Warehouse\")\n",
    "        self.running = False\n",
    "\n",
    "    def load_disk_partition(self, key):\n",
    "        \"\"\"\n",
    "        STEP 3: Load disk partition of size vP from R\n",
    "        Returns list of disk tuples related to the key\n",
    "        \"\"\"\n",
    "        disk_tuples = []\n",
    "        \n",
    "        # Load customer data for the key\n",
    "        if key in self.customer_master:\n",
    "            customer_data = self.customer_master[key]\n",
    "            disk_tuples.append(('customer', key, customer_data))\n",
    "        \n",
    "        # In full implementation, would load related product data etc.\n",
    "        # Limited to vP=500 tuples as specified\n",
    "        \n",
    "        return disk_tuples\n",
    "\n",
    "    def generate_join_output(self, stream_tuple, disk_tuple):\n",
    "        \"\"\"\n",
    "        STEP 4: Generate combined result by joining stream + disk data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            disk_type, key, disk_data = disk_tuple\n",
    "            \n",
    "            customer_id = stream_tuple['Customer_ID']\n",
    "            product_id = stream_tuple['Product_ID']\n",
    "            \n",
    "            # Get product data from disk relation R\n",
    "            product_info = self.product_master.get(product_id, {})\n",
    "            \n",
    "            # Calculate total amount\n",
    "            quantity = stream_tuple['quantity']\n",
    "            price = product_info.get('Price', 0)\n",
    "            total_amount = quantity * price\n",
    "            \n",
    "            # Create enriched record with ALL columns for FactSales\n",
    "            enriched = {\n",
    "                # Core transactional data\n",
    "                'Customer_ID': customer_id,\n",
    "                'Product_ID': product_id,\n",
    "                'Order_ID': stream_tuple['orderID'],\n",
    "                'Date': stream_tuple['date'],\n",
    "                'Quantity': quantity,\n",
    "                \n",
    "                # Enriched customer demographics (from DimCustomer)\n",
    "                'Gender': disk_data.get('Gender'),\n",
    "                'Age': disk_data.get('Age'),\n",
    "                'Occupation': disk_data.get('Occupation'),\n",
    "                'City_Category': disk_data.get('City_Category'),\n",
    "                'Stay_In_Current_City_Years': disk_data.get('Stay_In_Current_City_Years'),\n",
    "                'Marital_Status': disk_data.get('Marital_Status'),\n",
    "                \n",
    "                # Enriched product details (from DimProduct)\n",
    "                'Product_Category': product_info.get('Product_Category'),\n",
    "                'Price': price,\n",
    "                'StoreID': product_info.get('StoreID'),\n",
    "                'StoreName': product_info.get('StoreName'),\n",
    "                'SupplierID': product_info.get('SupplierID'),\n",
    "                'SupplierName': product_info.get('SupplierName'),\n",
    "                \n",
    "                # Calculated measures\n",
    "                'Total_Amount': total_amount\n",
    "            }\n",
    "            return enriched\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Join output error: {e}\")\n",
    "            return None\n",
    "\n",
    "    def cleanup_queue(self):\n",
    "        \"\"\"Remove keys from queue that have no more tuples in hash table\"\"\"\n",
    "        keys_to_remove = []\n",
    "        \n",
    "        for key in list(self.queue):\n",
    "            slot = self.hash_function(key)\n",
    "            if not any(entry['key'] == key for entry in self.hash_table[slot]):\n",
    "                keys_to_remove.append(key)\n",
    "        \n",
    "        for key in keys_to_remove:\n",
    "            while key in self.queue:\n",
    "                self.queue.remove(key)\n",
    "\n",
    "    def load_to_dw(self, enriched_data):\n",
    "        \"\"\"Load combined result to Data Warehouse fact table with ALL enriched columns\"\"\"\n",
    "        try:\n",
    "            self.cursor.execute(\"\"\"\n",
    "                INSERT INTO FactSales \n",
    "                (Customer_ID, Product_ID, Date_ID, Order_ID, Quantity,\n",
    "                 Gender, Age, Occupation, City_Category, Stay_In_Current_City_Years, Marital_Status,\n",
    "                 Product_Category, Price, StoreID, StoreName, SupplierID, SupplierName,\n",
    "                 Total_Amount)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\", (\n",
    "                # Core transactional data\n",
    "                enriched_data['Customer_ID'],\n",
    "                enriched_data['Product_ID'],\n",
    "                enriched_data['Date'],\n",
    "                enriched_data['Order_ID'],\n",
    "                enriched_data['Quantity'],\n",
    "                \n",
    "                # Enriched customer demographics\n",
    "                enriched_data['Gender'],\n",
    "                enriched_data['Age'],\n",
    "                enriched_data['Occupation'],\n",
    "                enriched_data['City_Category'],\n",
    "                enriched_data['Stay_In_Current_City_Years'],\n",
    "                enriched_data['Marital_Status'],\n",
    "                \n",
    "                # Enriched product details\n",
    "                enriched_data['Product_Category'],\n",
    "                enriched_data['Price'],\n",
    "                enriched_data['StoreID'],\n",
    "                enriched_data['StoreName'],\n",
    "                enriched_data['SupplierID'],\n",
    "                enriched_data['SupplierName'],\n",
    "                \n",
    "                # Calculated measures\n",
    "                enriched_data['Total_Amount']\n",
    "            ))\n",
    "            \n",
    "            # Commit in batches for performance (every 50 records)\n",
    "            if self.processed_count % 50 == 0:\n",
    "                self.conn.commit()\n",
    "                \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"DW load error: {e}\")\n",
    "            return False\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Run HYBRIDJOIN system with two independent threads as specified\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STARTING HYBRIDJOIN SYSTEM FOR WALMART DATA WAREHOUSE\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            # Thread 1: Continuous stream reader (independent thread)\n",
    "            stream_thread = threading.Thread(target=self.stream_reader_thread)\n",
    "            stream_thread.daemon = True\n",
    "            stream_thread.start()\n",
    "            \n",
    "            # Allow stream to populate initially\n",
    "            print(\"Waiting for stream data to populate...\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Thread 2: HYBRIDJOIN algorithm (main thread)\n",
    "            self.hybrid_join_algorithm()\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n HYBRIDJOIN stopped by user\")\n",
    "            self.running = False\n",
    "        except Exception as e:\n",
    "            print(f\"Error in HYBRIDJOIN execution: {e}\")\n",
    "        finally:\n",
    "            # Final cleanup\n",
    "            self.conn.commit()\n",
    "            self.conn.close()\n",
    "            print(\"HYBRIDJOIN system shutdown complete\")\n",
    "\n",
    "def main():\n",
    "    host = input(\"Enter database host [localhost]: \") or \"localhost\"\n",
    "    user = input(\"Enter database username [root]: \") or \"root\"\n",
    "    password = input(\"Enter database password: \")\n",
    "    \n",
    "    #host = \"localhost\"\n",
    "    #user = \"root\"\n",
    "    #password = \"yumn@1922@mn@\"\n",
    "    \n",
    "    db_creds = {\n",
    "        'host': host,\n",
    "        'user': user,\n",
    "        'password': password,\n",
    "        'database': 'walmart'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        hj = HYBRIDJOIN(db_creds, hS=10000, vP=500)\n",
    "        hj.run()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize HYBRIDJOIN: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c1b62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a83952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d30f56a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c91b261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
